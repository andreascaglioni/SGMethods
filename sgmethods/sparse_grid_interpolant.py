"""
The core module of SGmethods. It contains the implementation of sparse grid
interpolation in a class.
"""

from multiprocessing import Pool
import numpy as np
from numpy.linalg import norm as norm_l2
from sgmethods.tp_interpolants import TPPwLinearInterpolator
from sgmethods.nodes_tp import tp_knots
from sgmethods.tp_interpolant_wrapper import TPInterpolatorWrapper


class SGInterpolant:
    """Sparse grid interpolant class. It stores all relevant information to
    define it like multi-index set, 1D notes etc.
    It automatically computes the sparse grid and inclusion-exclusion
    coefficients upon initialization.
    It allows to interpolate high dimensinal functions on the sparse grid.
    """

    def __init__(
        self,
        mid_set,
        knots,
        lev2knots,
        tp_interpolant=TPPwLinearInterpolator,
        n_parallel=1,
        verbose=True,
    ):
        """Initialize data, compute inclusion-exclusion coeff.s, sparse grid

        Args:
            mid_set (numpy.ndarray[int]): 2D array. Multi-index set. NB must be
                downward closed.
            knots (Callable[[int], numpy.ndarray[double]]): Get the nodes vector
                with the given number of nodes.
            lev2knots (Callable[[int], int]): Given a level >=0, returns the
                corresponding number of nodes >0.
            tp_interpolant (Class TPInterpolant, optional): One of the classes in the module
                tp_interpolants. (e.g. TPPwLinearInterpolator). The
                user can also define their own class following the instructions
                in the same module. Defaults to a piecewise-linear inteprolant.
            n_parallel (int, optional): Number of parallel computations.
                Defaults to 1.
            verbose (bool, optional): Verbose output. Defaults to True.
        """
        self.verbose = verbose
        self.mid_set = mid_set  # Multi-index set
        self.card_mid_set = mid_set.shape[0]  # Cardinality multi-index set
        self.N = mid_set.shape[1]  # Dimensinality multi-index set
        # Knot function # NB need knots[1] = np.array([0.]) to increase # dim.s
        self.knots = knots
        # Level-to-knot function # NB need lev2knots(0)=1 to increase number of dim.s
        self.lev2knots = lev2knots
        self.tp_interpolant = tp_interpolant  # TP interpolant Class
        self.n_parallel = n_parallel

        # -------------- Members to compute in setup_interpolant ------------- #
        self.combination_coeffs = []  # Inclusio-exclusion coefficients. list(int)
        # Active multi-indices, i.e. those corresponding to a non-zero combination_coeff. list[np.ndarray[float]]
        self.active_mids = []
        # List of ``active TP nodes'', i.e. self.active_tp_nodes_list[i] are the nodes of the TP interpolant defined by self.active_mids[i]. self.active_tp_nodes_list[i] is generated by nodes_tp.tp_knots. list[tuple[np.ndarray[float]]]
        self.active_tp_nodes_list = []
        # For each TP interpolant defined by self.active_mids, the dimensions with more than 1 node (recall that if there is only 1 node, it is 0)
        self.active_tp_dims = []
        # Map each TP grid node to a correposnding SG node. It assumes that
        # Used to map collocations samples as: 
        #       f_on_TP = f_on_SG[self.map_curr_tp_to_SG[i], :]
        # list[numpy.ndarray[float]], map_tp_to_SG[i] has shape tuple(num_nodes_active_dirs)
        self.map_tp_to_SG = []
        self.setup_interpolant()

        # ------------------ Members to compute in setup_SG ------------------ #
        self.SG = []  # np array of shape (#colloc. pts, N)
        self.num_nodes = 0  # number of sparse grid nodes
        self.setup_SG()

    def setup_interpolant(self):
        """Computes and saves in class attributes some important quantities:
        combination coefficients, active multi-indicies, active TP dimensions,
        active TP nodes, map TP to SG nodes based on the multi-index set, the
        nodes and the level-to-knot function.

        Args:
            None

        Returns:
            None
        """
        bookmarks = np.unique(self.mid_set[:, 0], return_index=True)[1]
        bk = np.hstack(
            (bookmarks[2:], np.array([self.card_mid_set, self.card_mid_set]))
        )
        for n in range(self.card_mid_set):
            current_mid = self.mid_set[n, :]
            combin_coeff = 1
            # index of the 1st mid with 1st components = currentMid[0]+2 (dont
            # need to itertate over it or any following one)
            range_ids = bk[current_mid[0]]
            # NB in np slice start:stop, stop is NOT included!!
            mids_difference = self.mid_set[(n + 1) : (range_ids), :] - current_mid
            is_binary_vector = np.all(
                np.logical_and(mids_difference >= 0, mids_difference <= 1), axis=1
            )
            binary_rows = mids_difference[is_binary_vector]
            combi_elementary = np.power(-1, np.sum(binary_rows, axis=1))
            combin_coeff += np.sum(combi_elementary)

            if combin_coeff != 0:
                self.combination_coeffs.append(combin_coeff)
                self.active_mids.append(current_mid)
                num_nodes_dir = self.lev2knots(current_mid).astype(int)

                active_tp_dims = np.where(
                    num_nodes_dir > 1
                )  # TODO instead use: np.asarray(num_nodes_dir > 1).nonzero()

                self.active_tp_dims.append(active_tp_dims[0])
                num_nodes_active_dirs = num_nodes_dir[active_tp_dims]
                if num_nodes_active_dirs.shape[0] == 0:
                    num_nodes_active_dirs = np.array([1])
                self.active_tp_nodes_list.append(
                    tp_knots(self.knots, num_nodes_active_dirs)
                )

                # For each *active* tensor product interpolant (i.e. non-zero inclusion-exclusion coefficient), relate the index of each TP node to an index of the sparse grid self.SG.
                # NB Here self.map_tp_to_SG is
                # List[np.ndarray[int]].
                #
                shp = np.ndarray(tuple(num_nodes_active_dirs), dtype=int)
                self.map_tp_to_SG.append(shp)  # only allocated

    def setup_SG(self):
        """Computes and saves in class attributes some important quantities:
        SG (sparse grid), num_nodes, map_tp_to_SG
        based on active_tp_nodes_list.
        """

        SG = np.array([]).reshape((0, self.N))
        for n, curr_active_tp_nodes in enumerate(self.active_tp_nodes_list):
            curr_active_dims = self.active_tp_dims[n]
            # NB "*" is "unpacking" operator (return comma-separated list)
            mesh_grid = np.meshgrid(*curr_active_tp_nodes, indexing="ij")
            it = np.nditer(mesh_grid[0], flags=["multi_index"])
            for x in it:
                curr_node_active_dims = [
                    mesh_grid[j][it.multi_index] for j in range(len(mesh_grid))
                ]
                # complete it with 0s in inactive dimensions
                curr_node = np.zeros(self.N)
                curr_node[curr_active_dims] = curr_node_active_dims
                # check = np.where(~(SG-currNode).any(axis=1))[0]
                check = np.where(np.sum(np.abs(SG - curr_node), axis=1) < 1.0e-10)[0]
                found = check.shape[0]
                # if currNode was in the SG more than once, something wrong
                assert found <= 1
                if found:  # if found, add the index to mapTPtoSG[n]
                    self.map_tp_to_SG[n][it.multi_index] = check[0]
                else:  # if not found, add it to sparse grid, add to mapTPtoSG
                    SG = np.vstack((SG, np.array(curr_node)))
                    self.map_tp_to_SG[n][it.multi_index] = SG.shape[0] - 1
        self.SG = SG
        self.num_nodes = SG.shape[0]

    # TODO remove dimF from signature; check all other scripts
    def sample_on_SG(self, f, dim_f=None, old_xx=None, old_samples=None):
        """Sample a given function on the sparse grid.

        Optionally recycle previous samples stored e.g. from a previous
        interpolation. The class also takes care automatically of the case in
        which the sparse grid has increased dimension (number of approcimated
        scaar parameters).
        The class first checks whether or not there is anything to recycle, then
        it sample new values.

        NB The class assumes ``f`` takes as input a numpy.ndarray[float] of
        parameters. The 1st output is assumed to be a numpy.ndarray[float] or a
        float (in this case, it is transformed into a 1-entry
        numpy.ndarray[float]).

        Args:
            f (Callable[[numpy.ndarray[float]], numpy.ndarray[float]]): The
                function to interpolate.
            dim_f (int, optional): Dimensinoality codomain of ``f``. Defaults to
                None.
            old_xx (numpy.ndarray[float], optional): 2D array.  Each row is a
                parametric point Defaults to None.
            old_samples (numpy.ndarray[float], optional): 2D array. Each row
                corresponds to a row of old_xx. Defaults to None.

        Returns:
            numpy.ndarray[float]: 2D array. Each row
            is the value of ``f`` on a sparse grid (``self.SG``) point.
        """

        # Sanity checks: dimensions oldXx, oldSamples
        if not (old_xx is None and old_samples is None):
            assert old_xx.shape[0] == old_samples.shape[0]

        # Sample first node to allocate output array
        # NB cannot use old_xx becuase dimensioanlity may ahve changed and
        # old_xx will need to be either truncated or 0-extended in dimension 1
        f_y0 = np.atleast_1d(f(self.SG[0]))  # turn in array if scalar
        assert len(f_y0.shape) == 1
        dim_f = f_y0.size
        f_on_SG = np.zeros((self.num_nodes, dim_f))
        f_on_SG[0, :] = f_y0

        # If old_xx not None, use it to find dimensions of f samples
        # assert  self.SG.shape[0] > 0
        # if not old_samples is None:
        #     old_samples = np.atleast_1d(old_samples)
        #     dim_f = old_samples.shape[1]
        #     assert dim_f >= 1
        # else:  # Sample on first SG node to determine dimension of f samples
        #     node0 = self.SG[0]
        #     f_on_SG0 = np.atleast_1d(f(node0))  # turn in array if scalar
        #     dim_f = f_on_SG0.size
        #     # Save the computed value!
        #     old_xx = np.atleast_2d(node0)
        #     old_samples = f_on_SG0
        # f_on_SG = np.zeros((self.num_nodes, dim_f))  # the return array

        # Turn old_xx and old_samples into np arrays of size 0, if None
        if (old_xx is None) or (old_samples is None):
            old_xx = np.zeros((0, self.N))
            old_samples = np.zeros((0, dim_f))

        # If old_xx has smaller dimension than the sparse grid: 0-extend it
        if old_xx.shape[1] < self.SG.shape[1]:
            filler = np.zeros((old_xx.shape[0], self.SG.shape[1] - old_xx.shape[1]))
            old_xx = np.hstack((old_xx, filler))

        # If old_xx has larger dimension than the sparse grid: Truncate it
        elif old_xx.shape[1] > self.SG.shape[1]:
            curr_dim = self.N
            tail_norm = norm_l2(old_xx[:, curr_dim::], ord=1, axis=1).astype(int)
            # Can recycle old sample only if removed entries=0. Discart others.
            idxs_valid_entries = np.where(tail_norm == 0)[0]
            old_xx = old_xx[idxs_valid_entries, 0 : self.N]
            old_samples = old_samples[idxs_valid_entries]

        # Find sparse grid points to either load from load_xx or to sample now
        # Update n_recycle  every time a useful node found in old_xx
        idxs_nodes_to_compute = []  # list indices SG nodes to compute on
        n_recycle = 0
        for n in range(1, self.num_nodes):  # NB 1 becuase node 0 sampled above!
            curr_node = self.SG[n, :]
            # Check if curr_node in old_xx
            check = np.where(norm_l2(old_xx - curr_node, 1, axis=1) < 1.0e-10)[0]
            if check.size > 0:  # found
                n_recycle += 1
            else:
                idxs_nodes_to_compute.append(n)

        # Sample (possibily in parallel) on points where no value is recycled.
        if len(idxs_nodes_to_compute) > 0:
            # Split, even if could instead run with 1 Pool instance (solution if
            # Pool not installed)
            if self.n_parallel == 1:
                for idx in idxs_nodes_to_compute:
                    f_on_SG[idx, :] = f(self.SG[idx])
            elif self.n_parallel > 1:
                pool = Pool(self.n_parallel)
                tmp = np.array(pool.map(f, idxs_nodes_to_compute))
                pool.close()
                pool.join()  # TODO should not join before closing?
                if len(tmp.shape) == 1:
                    tmp = tmp.reshape((-1, 1))
                f_on_SG[idxs_nodes_to_compute, :] = tmp
            else:
                raise ValueError('self.NParallel not int >= 1"')
        if self.verbose:
            print(
                "Recycled",
                n_recycle,
                "; Discarted",
                old_xx.shape[0] - n_recycle,
                "; Sampled",
                self.SG.shape[0] - n_recycle,
            )
        return f_on_SG

    def interpolate(self, x_new, f_on_SG):
        """Evaluate the interpolant on new parametric points.

        Args:
            x_new (numpy.ndarray[float]): 2D array. New parametric points where
                to interpolate. Each row is a point.
            f_on_SG (numpy.ndarray[float]): 2D array. Values of function on the
                sparse grid. Each row is a value corresponding to a parametric
                point in the sparse grid ``self.SG``.

        Returns:
            numpy.ndarray[float]: 2D array. Values of the interpolant of f on
            ``x_new``. Each row is a value corresponds to the parameter stored
            in the same row of ``x_new``.
        """

        out = np.zeros((x_new.shape[0], f_on_SG.shape[1]))
        for n, mid in enumerate(self.active_mids):
            curr_active_nodes_tuple = self.active_tp_nodes_list[n]
            curr_active_dims = self.active_tp_dims[n]
            map_curr_tp_to_SG = self.map_tp_to_SG[n]
            # output is a matrix of shape = shape(mapCurrTPtoSG) + (dimF,)
            f_on_curr_tp_grid = f_on_SG[map_curr_tp_to_SG, :]
            tp_interpolant = TPInterpolatorWrapper(
                curr_active_nodes_tuple,
                curr_active_dims,
                f_on_curr_tp_grid,
                self.tp_interpolant,
            )
            out = out + self.combination_coeffs[n] * tp_interpolant(x_new)
        return np.squeeze(out)  # remove dimensions of length 1
